# временные предложения

[04.25.2020 21:52]
вот термины, которые они используют в машинном обучении:

# Любит/не любит
- loss - насколько предсказание отличается от нужного результата. В случае для списка пар `(x, y)` часто используется среднеквадратичная ошибка. Пусть `y` — это судьбой предначертанное в зависимости от `x`. Пусть `f(x)` пытается угадать, какая судьба `y` ожидаеет `x`, `square` возводит число в квадрат, тогда `square(f(x) - y)` показывает насколько вангование точно (если 0 — значит сбудется, чем больше это число — тем хуже ванга `f(x)` нагадала для `y`). Задача машинного обучния — сделать вангование более точным, в этих терминах нужно сделать `square(f(x) - y)` как можно меньше. Для двух пар данных `(x1, y1) (x2, y2)` будет так: `square(f(x1) - y1) + square(f(x2) - y2)` - чем меньше это число, тем точнее прогноз для двух примеров. Для `N` пар данных будет так:
```ts
const loss = (x: number, y: number): number => square(f(x) - y)
const sum = (list: number[]): number => list.reduce((acc, element) => acc + element, 0)

const lossAll = sum([[x1, y1], [x2, y2], ..., [x, y]].map(([x, y]) => loss(x, y)))
// хочется, чтобы lossAll стал близко к 0
```


пусть **f'(x)** - производная **f(x)**
1. `f'(x) > 0`, функция возрастает
2. `f'(x) < 0`, функция убывает
3. `f'(x) == 0`, функция не возрастает и не убывает (прямая линия на графике)
4. `f'(x) == 0` значит `f(x)` достигает максимального значения или минимального.


- что такое дифференциал?
- что такое [градиент](https://en.wikipedia.org/wiki/Gradient), градиентный спуск (куда что спускается?)
- как посчитать градиент для матрицы (с весами нейронов слоя) (для функции от нескольких переменных это вектор с частными производными по всем переменным)



- почему в йога-сутре патанджали большинство русскоязычных переводов дают определение йоги (chitta vritti nirodha) как "остановки деятельности сознания"? 
