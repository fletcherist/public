# временные предложения

[04.25.2020 21:52]
вот термины, которые они используют в машинном обучении:

# Любит/не любит
- loss - насколько предсказание отличается от нужного результата. В случае для списка пар `(x, y)` часто используется среднеквадратичная ошибка. Пусть `y` — это судьбой предначертанное в зависимости от `x`. Пусть `f(x)` пытается угадать, какая судьба `y` ожидаеет `x`, `square` возводит число в квадрат, тогда `square(f(x) - y)` показывает насколько вангование точно (если 0 — значит сбудется, чем больше это число — тем хуже ванга `f(x)` нагадала для `y`). Задача машинного обучния — сделать вангование более точным, в этих терминах нужно сделать `square(f(x) - y)` как можно меньше. Для двух пар данных `(x1, y1) (x2, y2)` будет так: `square(f(x1) - y1) + square(f(x2) - y2)` - чем меньше это число, тем точнее прогноз для двух примеров. Для `N` пар данных будет так:
```ts
const loss = (x: number, y: number): number => square(f(x) - y)
const sum = (list: number[]): number => list.reduce((acc, element) => acc + element, 0)

const lossAll = sum([[x1, y1], [x2, y2], ..., [x, y]].map(([x, y]) => loss(x, y)))
// хочется, чтобы lossAll был как можно меньше
```

пусть **f'(x)** - производная **f(x)**
1. `f'(x) > 0`, функция возрастает
2. `f'(x) < 0`, функция убывает
3. `f'(x) == 0`, функция не возрастает и не убывает (прямая линия на графике)
4. `f'(x) == 0` значит `f(x)` достигает максимального значения или минимального.

- что такое дифференциал?
- что такое [градиент](https://en.wikipedia.org/wiki/Gradient), градиентный спуск (куда что спускается?)
- как посчитать градиент для матрицы (с весами нейронов слоя) (для функции от нескольких переменных это вектор с частными производными по всем переменным)


# loss
`f(x)` is an инференс

mean absolute error - средняя ошибка. нормас для регрессии
```
const loss = (x: predict[], y: output[]): number => 1 / x.length * compose(sum,  map((x, i) => f(x) - y[i]))(x)
```
mean squared error - средняя ошибка в квадрате. супер топ для регрессии
```
const loss = (x: predict[], y: output[]): number => 1 / x.length * compose(sum, squared, map((x, i) => f(x) - y[i]))(x)
```

binary cross-entropy
```
from math import log
 
# calculate binary cross entropy
def binary_cross_entropy(actual, predicted):
	sum_score = 0.0
	for i in range(len(actual)):
		sum_score += actual[i] * log(1e-15 + predicted[i])
	mean_sum_score = 1.0 / len(actual) * sum_score
	return -mean_sum_score
```


- почему в йога-сутре патанджали большинство русскоязычных переводов дают определение йоги (chitta vritti nirodha) как "остановки деятельности сознания"?



домен — то, в чём читта находит своё выражение

- дхарана на домене ведёт к переходу читты в этот домен.

Например, я переезжаю из россии в страну, где говорят на английском языке, и если я нахожусь там продолжительно долго, читта будет постепенно окрашиваться в слова английского языка, пока мышление на русском языке полностью не заместится мышлением на английском языке. После длительного чтения стихов на русском языке, читта выражается в строках, рифмующихся между собой. Среди прочего, интересным оказался шахматный домен, который был открыт случайно после нескольких дней непрерывной игры по несколько часов ежедневно — поток читты тогда был речью на русском языке, перемешиваясь иногда с образом случайной позиции на доске, где автоматически просчитывались варианты, подобно тому, как я делаю это во время игры с реальной позицией. Ещё один интересный домен — баховский домен, куда читта нередко попадает после продолжительного непрерывного прослушивания однообразной музыки, например органная 10 часов нон-стоп; выражается несколькими автосгенереннымы контрапунктных subject, которые как-то общаются между собой.

- «я» является функцией читты


cheeses


